{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to loadinto a DataFrame\n",
    "# Y1.csv doesn’t have a header so\n",
    "# add one when loading the file\n",
    "X1 = pd.read_csv(\"data/X1.csv\")\n",
    "Y1 = pd.read_csv(\"data/Y1.csv\", header=None, names=['revenue'])\n",
    "\n",
    "\n",
    "# ENLEVER colonne \"Unnamed\" du dataset : utilité ? -> Pas listé dans les features du pdf\n",
    "X = X1.drop(['Unnamed: 0', 'img_url', 'description'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(X.describe())\\n\\nplt.figure(figsize=(7,4))\\ncorr = X.corr(numeric_only=True)\\nheatmap = sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\\'.1g\\')\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(X.describe())\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "corr = X.corr(numeric_only=True)\n",
    "heatmap = sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt='.1g')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating empty DataFrame to start\n",
    "\"\"\"\n",
    "n_samples = X.shape[0]\n",
    "data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keeping the directly usable features\n",
    "\"\"\"\n",
    "def get_directrly_usable_features(df):\n",
    "    directly_usable_features = [\"ratings\", \"n_votes\", \"is_adult\"]\n",
    "    for feature in directly_usable_features:\n",
    "        df[feature] = X[feature]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"production_year\" feature\n",
    "\"\"\"\n",
    "\n",
    "def get_prod_year_feature(df, params):\n",
    "    \n",
    "    style = params[\"production_year_style\"] # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
    "    if style != \"no_period\" :\n",
    "        n_year_period = params[\"n_year_period\"]\n",
    "\n",
    "    # Removing previously computed categorie(s) for the \"production_year\" initial feature\n",
    "\n",
    "    for feature in df.columns:\n",
    "        if len(feature) >= 8 and (feature[:6] == \"period\" or feature == \"production_year\"):\n",
    "            df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "    # Creating new categorie(s) for the \"production_year\" initial feature\n",
    "\n",
    "    prod_year = X[\"production_year\"].copy()\n",
    "    if style == \"per_quantile\" or style == \"per_period_length\":\n",
    "        categories = np.ones((n_year_period, n_samples))\n",
    "\n",
    "        if style == \"per_quantile\":\n",
    "            thresholds = prod_year.quantile(np.arange(1, n_year_period) / n_year_period)\n",
    "        else :\n",
    "            thresholds = np.min(prod_year) + (np.max(prod_year) - np.min(prod_year))*np.arange(1, n_year_period)/n_year_period\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            categories[i+1] = (prod_year >= threshold).astype(int)\n",
    "            categories[i] -= categories[i+1]\n",
    "        for period in range(n_year_period):\n",
    "            df[\"period {}\".format(period)] = categories[period]\n",
    "    elif style == \"no_period\":\n",
    "        df[\"production_year\"] = prod_year\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"runtime\" feature\n",
    "\n",
    "The problem is here that we have some missing values, we have to deal with it.\n",
    "\"\"\"\n",
    "\n",
    "def get_runtime_feature(df, params):\n",
    "\n",
    "    # Add other smarter ways ?\n",
    "    \n",
    "    replace_type = params[\"runtime_replace_type\"] # \"zero\" / \"mean\"\n",
    "    \n",
    "    runtime = X[\"runtime\"].copy()\n",
    "    if replace_type == \"zero\":\n",
    "        runtime[runtime == \"\\\\N\"] = 0\n",
    "    if replace_type == \"mean\":\n",
    "        mean = np.mean(runtime[runtime != \"\\\\N\"].astype(float))\n",
    "        runtime[runtime == \"\\\\N\"] = mean\n",
    "    df[\"runtime\"] = runtime.astype(float)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\"\"\"\n",
    "Dealing with the \"studio\" feature\n",
    "\n",
    "\n",
    "Juste rajouter toute les features une par une me semblait un peu lourd (yen a 509), ducoup j'effectue PCA dessus.\n",
    "Jsp si ça se fait ? (on peut changer l'algo de dimensionality reduction aussi si on veut)\n",
    "\"\"\"\n",
    "\n",
    "def get_studio_feature(df, params):\n",
    "    use_PCA = params[\"studio_use_PCA\"]\n",
    "    if use_PCA :\n",
    "        dim = params[\"studio_PCA_dim\"]\n",
    "\n",
    "    # Removing previously computed categorie(s) for the \"studio\" initial feature\n",
    "    for feature in df.columns:\n",
    "        if len(feature) >= 10 and feature[:10] == \"studio_PC_\":\n",
    "            df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "    # Creating new categorie(s) for the \"studio\" initial feature\n",
    "    studio = copy.deepcopy(X[\"studio\"])\n",
    "    studio_labels = np.unique(studio)\n",
    "    studio_features = np.zeros((len(studio_labels), n_samples))\n",
    "    for i, label in enumerate(studio_labels) :\n",
    "        studio_features[i] = (studio == label).astype(int)\n",
    "    \n",
    "    # Applying pca or not\n",
    "    if use_PCA :\n",
    "        pca = PCA(n_components=dim)\n",
    "        out = pca.fit_transform(studio_features.T)\n",
    "    else :\n",
    "        normals = studio_features[np.count_nonzero(studio_features, axis=1) > 5].T\n",
    "        outliers = np.sum(studio_features[np.count_nonzero(studio_features, axis=1) <= 5].T, axis = 1)\n",
    "        out = np.zeros((normals.shape[0], normals.shape[1] + 1))\n",
    "        out[:, :-1] = normals\n",
    "        out[:, -1] = outliers\n",
    "        dim = out.shape[1]\n",
    "    \n",
    "    df[[\"studio_PC_{}\".format(i) for i in range(dim)]] = out\n",
    "    return df\n",
    "\n",
    "# Ya plein de warnings quand dim trop grand ou pas de PCA /: \n",
    "# jsp comment regler ça... en utilisant pd.concat ça tourne vraiment extrêmement lentement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"genres\" feature\n",
    "\n",
    "Je rajoute juste une feature par genre, j'espère ça suffit ? -> pt-être dimension reduction sur ça aussi ?\n",
    "Certains films ont pas de genre attitré (genre = \"\\\\N\"),\n",
    "ce que j'ai fait ici considère ça comme un genre à part entière, ptet on peut deal le truc autrement ?\n",
    "\"\"\"\n",
    "\n",
    "def get_genre_feature(df):\n",
    "\n",
    "    X.loc[X[\"genres\"] == \"\\\\N\", \"genres\"] = \"Others\"\n",
    "    all_genres = X[\"genres\"].copy()\n",
    "    diff_genres = []\n",
    "\n",
    "    for genres in np.unique(all_genres):\n",
    "        for genre in genres.split(\",\") :\n",
    "            if not genre in diff_genres :\n",
    "                diff_genres.append(genre)\n",
    "\n",
    "    for genre in diff_genres:\n",
    "        df[genre] = [1 if genre in genres.split(\",\") else 0 for genres in all_genres]  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"text_embeddings\" feature\n",
    "\n",
    "\n",
    "Dimension of embedding space is too high -> dimensionnality reduction\n",
    "J'utilise que PCA pr l'instant ici aussi\n",
    "\"\"\"\n",
    "def get_text_embedding_feature(df, params):\n",
    "    \n",
    "    output_dim = params[\"text_embedding_PCA_dim\"] # output dimension of PCA\n",
    "\n",
    "    # Removing previously computed categorie(s) for the \"text_embedding\" initial feature\n",
    "    for feature in df.columns:\n",
    "        if len(feature) >= 18 and feature[:18] == \"text_embedding_PC_\":\n",
    "            df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "    # Creating new categorie(s) for the \"text_embedding\" initial feature\n",
    "    text_embeddings = X[\"text_embeddings\"]\n",
    "    input_dim = 768\n",
    "    embeddings = np.zeros((n_samples, input_dim))\n",
    "    for i, text_embedding in enumerate(text_embeddings):\n",
    "        embeddings[i] = list(map(float,text_embedding[1:-1].split(\",\")))\n",
    "\n",
    "    # applying PCA\n",
    "    pca = PCA(n_components=output_dim)\n",
    "    output = pca.fit_transform(embeddings)\n",
    "\n",
    "    df[[\"text_embedding_PC_{}\".format(i) for i in range(output_dim)]] = output\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"img_embeddings\" feature\n",
    "\n",
    "\n",
    "Dimension of embedding space is too high -> dimensionnality reduction\n",
    "J'utilise que PCA pr l'instant ici aussi\n",
    "\"\"\"\n",
    "def get_img_embedding_feature(df, params):\n",
    "    output_dim = params[\"img_embedding_PCA_dim\"] # output dimension of PCA\n",
    "\n",
    "    # Removing previously computed categorie(s) for the \"img_embedding\" initial feature\n",
    "    for feature in df.columns:\n",
    "        if len(feature) >= 17 and feature[:17] == \"img_embedding_PC_\":\n",
    "            df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "    # Creating new categorie(s) for the \"img_embedding\" initial feature\n",
    "    img_embeddings = X[\"img_embeddings\"]\n",
    "    input_dim = 2048\n",
    "    embeddings = np.zeros((n_samples, input_dim))\n",
    "    for i, img_embedding in enumerate(img_embeddings):\n",
    "        embeddings[i] = list(map(float,img_embedding[1:-1].split(\",\")))\n",
    "\n",
    "    # applying PCA\n",
    "    pca = PCA(n_components=output_dim)\n",
    "    output = pca.fit_transform(embeddings)\n",
    "\n",
    "    df[[\"img_embedding_PC_{}\".format(i) for i in range(output_dim)]] = output\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessed(params):\n",
    "    df = pd.DataFrame()\n",
    "    df = get_genre_feature(df)\n",
    "    df = get_directrly_usable_features(df)\n",
    "    df = get_prod_year_feature(df, params)\n",
    "    df = get_studio_feature(df, params)\n",
    "    df = get_runtime_feature(df, params)\n",
    "    df = get_text_embedding_feature(df, params)\n",
    "    df = get_img_embedding_feature(df, params)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessed_dict(params):\n",
    "    final = {}\n",
    "    year_dict = {}\n",
    "    for val1 in params[\"production_year_style\"] :\n",
    "        for val2 in params[\"n_year_period\"] :\n",
    "            tmp = {}\n",
    "            tmp[\"production_year_style\"] = val1\n",
    "            tmp[\"n_year_period\"] = val2\n",
    "            year_dict[val1+str(val2)] = get_prod_year_feature(pd.DataFrame(), tmp)\n",
    "    final[\"year\"] = year_dict\n",
    "    \n",
    "    studio_dict = {}\n",
    "    for val1 in params[\"studio_use_PCA\"] :\n",
    "        for val2 in params[\"studio_PCA_dim\"] :\n",
    "            tmp = {}\n",
    "            tmp[\"studio_use_PCA\"] = val1\n",
    "            tmp[\"studio_PCA_dim\"] = val2\n",
    "            studio_dict[str(val1)+str(val2)] = get_studio_feature(pd.DataFrame(), tmp)\n",
    "    final[\"studio\"] = studio_dict\n",
    "    \n",
    "    runtime_dict = {}\n",
    "    for val in params[\"runtime_replace_type\"] :\n",
    "        tmp = {}\n",
    "        tmp[\"runtime_replace_type\"] = val\n",
    "        runtime_dict[val] = get_runtime_feature(pd.DataFrame(), tmp)\n",
    "    final[\"runtime\"] = runtime_dict\n",
    "    \n",
    "    text_dict = {}\n",
    "    for val in params[\"text_embedding_PCA_dim\"] :\n",
    "        tmp = {}\n",
    "        tmp[\"text_embedding_PCA_dim\"] = val\n",
    "        text_dict[val] = get_text_embedding_feature(pd.DataFrame(), tmp)\n",
    "    final[\"text\"] = text_dict\n",
    "    \n",
    "    img_dict = {}\n",
    "    for val in params[\"img_embedding_PCA_dim\"] :\n",
    "        tmp = {}\n",
    "        tmp[\"img_embedding_PCA_dim\"] = val\n",
    "        img_dict[val] = get_img_embedding_feature(pd.DataFrame(), tmp)\n",
    "    final[\"img\"] = img_dict   \n",
    "    final[\"genre\"] = get_genre_feature(pd.DataFrame())\n",
    "    final[\"base\"] = get_directrly_usable_features(pd.DataFrame())\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessed_from_dict(params_dict, params):\n",
    "    frames = []\n",
    "    frames.append(params_dict[\"genre\"])\n",
    "    frames.append(params_dict[\"base\"])\n",
    "    frames.append(params_dict[\"year\"][params[\"production_year_style\"] + str(params[\"n_year_period\"])])\n",
    "    frames.append(params_dict[\"studio\"][str(params[\"studio_use_PCA\"]) + str(params[\"studio_PCA_dim\"])])\n",
    "    frames.append(params_dict[\"runtime\"][params[\"runtime_replace_type\"]])\n",
    "    frames.append(params_dict[\"text\"][params[\"text_embedding_PCA_dim\"]])\n",
    "    frames.append(params_dict[\"img\"][params[\"img_embedding_PCA_dim\"]])\n",
    "    return pd.concat(frames, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of data preprocessing\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"production_year_style\" : \"per_quantile\", # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
    "    \"n_year_period\" : 5,\n",
    "    \"runtime_replace_type\" : \"mean\", # \"mean\" / \"zero\"\n",
    "    \"studio_use_PCA\" : True,\n",
    "    \"studio_PCA_dim\" : 50,\n",
    "    \"text_embedding_PCA_dim\" : 50,\n",
    "    \"img_embedding_PCA_dim\" : 50\n",
    "}\n",
    "\n",
    "preprocessed_data = create_preprocessed(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {\n",
    "    \"production_year_style\" : [\"per_quantile\", \"per_period_length\", \"no_period\"], # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
    "    \"n_year_period\" : [3, 5, 10],\n",
    "    \"runtime_replace_type\" : [\"mean\", \"zero\"], # \"mean\" / \"zero\"\n",
    "    \"studio_use_PCA\" : [True],\n",
    "    \"studio_PCA_dim\" : [1, 10],\n",
    "    \"text_embedding_PCA_dim\" : [1, 10],\n",
    "    \"img_embedding_PCA_dim\" : [1, 10]\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"production_year_style\" : \"per_quantile\", # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
    "    \"n_year_period\" : 5,\n",
    "    \"runtime_replace_type\" : \"mean\", # \"mean\" / \"zero\"\n",
    "    \"studio_use_PCA\" : True,\n",
    "    \"studio_PCA_dim\" : 1,\n",
    "    \"text_embedding_PCA_dim\" : 10,\n",
    "    \"img_embedding_PCA_dim\" : 10\n",
    "}\n",
    "\n",
    "preprocessed_dict = create_preprocessed_dict(all_params)\n",
    "preprocessed_data = create_preprocessed_from_dict(preprocessed_dict, params)\n",
    "preprocessed_data_2 = create_preprocessed(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Family</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>History</th>\n",
       "      <th>...</th>\n",
       "      <th>img_embedding_PC_0</th>\n",
       "      <th>img_embedding_PC_1</th>\n",
       "      <th>img_embedding_PC_2</th>\n",
       "      <th>img_embedding_PC_3</th>\n",
       "      <th>img_embedding_PC_4</th>\n",
       "      <th>img_embedding_PC_5</th>\n",
       "      <th>img_embedding_PC_6</th>\n",
       "      <th>img_embedding_PC_7</th>\n",
       "      <th>img_embedding_PC_8</th>\n",
       "      <th>img_embedding_PC_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.256436</td>\n",
       "      <td>-6.489770</td>\n",
       "      <td>-3.349808</td>\n",
       "      <td>-1.176245</td>\n",
       "      <td>5.893610</td>\n",
       "      <td>-0.582044</td>\n",
       "      <td>-1.161454</td>\n",
       "      <td>2.123083</td>\n",
       "      <td>-5.272238</td>\n",
       "      <td>0.114909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.427690</td>\n",
       "      <td>-1.379267</td>\n",
       "      <td>-3.140826</td>\n",
       "      <td>-1.040603</td>\n",
       "      <td>-7.670217</td>\n",
       "      <td>4.403435</td>\n",
       "      <td>1.477009</td>\n",
       "      <td>-4.139292</td>\n",
       "      <td>3.953297</td>\n",
       "      <td>-0.370849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.052495</td>\n",
       "      <td>4.428169</td>\n",
       "      <td>-0.752572</td>\n",
       "      <td>-4.224751</td>\n",
       "      <td>0.135466</td>\n",
       "      <td>-6.221801</td>\n",
       "      <td>-0.616367</td>\n",
       "      <td>-2.482779</td>\n",
       "      <td>-0.894261</td>\n",
       "      <td>1.929961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.471225</td>\n",
       "      <td>-1.694614</td>\n",
       "      <td>-0.049679</td>\n",
       "      <td>1.635100</td>\n",
       "      <td>4.501932</td>\n",
       "      <td>-1.861986</td>\n",
       "      <td>1.262274</td>\n",
       "      <td>3.071732</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.814752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.992412</td>\n",
       "      <td>-0.808753</td>\n",
       "      <td>2.917086</td>\n",
       "      <td>-3.761091</td>\n",
       "      <td>-0.365959</td>\n",
       "      <td>2.251730</td>\n",
       "      <td>0.507456</td>\n",
       "      <td>-2.131749</td>\n",
       "      <td>0.284576</td>\n",
       "      <td>0.237786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Action  Adventure  Animation  Comedy  Crime  Documentary  Drama  Family  \\\n",
       "0       0          1          0       1      0            0      1       0   \n",
       "1       1          0          0       0      1            0      1       0   \n",
       "2       0          0          0       0      0            0      1       1   \n",
       "3       0          0          0       0      0            0      1       0   \n",
       "4       0          0          0       0      1            0      1       0   \n",
       "\n",
       "   Fantasy  History  ...  img_embedding_PC_0  img_embedding_PC_1  \\\n",
       "0        0        0  ...           -3.256436           -6.489770   \n",
       "1        0        0  ...           -5.427690           -1.379267   \n",
       "2        0        0  ...            2.052495            4.428169   \n",
       "3        0        0  ...           -0.471225           -1.694614   \n",
       "4        0        0  ...           -6.992412           -0.808753   \n",
       "\n",
       "   img_embedding_PC_2  img_embedding_PC_3  img_embedding_PC_4  \\\n",
       "0           -3.349808           -1.176245            5.893610   \n",
       "1           -3.140826           -1.040603           -7.670217   \n",
       "2           -0.752572           -4.224751            0.135466   \n",
       "3           -0.049679            1.635100            4.501932   \n",
       "4            2.917086           -3.761091           -0.365959   \n",
       "\n",
       "   img_embedding_PC_5  img_embedding_PC_6  img_embedding_PC_7  \\\n",
       "0           -0.582044           -1.161454            2.123083   \n",
       "1            4.403435            1.477009           -4.139292   \n",
       "2           -6.221801           -0.616367           -2.482779   \n",
       "3           -1.861986            1.262274            3.071732   \n",
       "4            2.251730            0.507456           -2.131749   \n",
       "\n",
       "   img_embedding_PC_8  img_embedding_PC_9  \n",
       "0           -5.272238            0.114909  \n",
       "1            3.953297           -0.370849  \n",
       "2           -0.894261            1.929961  \n",
       "3            0.000716            0.814752  \n",
       "4            0.284576            0.237786  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Family</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>History</th>\n",
       "      <th>...</th>\n",
       "      <th>img_embedding_PC_0</th>\n",
       "      <th>img_embedding_PC_1</th>\n",
       "      <th>img_embedding_PC_2</th>\n",
       "      <th>img_embedding_PC_3</th>\n",
       "      <th>img_embedding_PC_4</th>\n",
       "      <th>img_embedding_PC_5</th>\n",
       "      <th>img_embedding_PC_6</th>\n",
       "      <th>img_embedding_PC_7</th>\n",
       "      <th>img_embedding_PC_8</th>\n",
       "      <th>img_embedding_PC_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.256434</td>\n",
       "      <td>-6.489767</td>\n",
       "      <td>-3.349805</td>\n",
       "      <td>-1.176368</td>\n",
       "      <td>5.894304</td>\n",
       "      <td>-0.583349</td>\n",
       "      <td>-1.162693</td>\n",
       "      <td>2.124027</td>\n",
       "      <td>-5.306920</td>\n",
       "      <td>0.080211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.427684</td>\n",
       "      <td>-1.379301</td>\n",
       "      <td>-3.140865</td>\n",
       "      <td>-1.040555</td>\n",
       "      <td>-7.670521</td>\n",
       "      <td>4.405017</td>\n",
       "      <td>1.473135</td>\n",
       "      <td>-4.142650</td>\n",
       "      <td>3.978261</td>\n",
       "      <td>-0.383656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.052491</td>\n",
       "      <td>4.428200</td>\n",
       "      <td>-0.752544</td>\n",
       "      <td>-4.224905</td>\n",
       "      <td>0.135836</td>\n",
       "      <td>-6.222792</td>\n",
       "      <td>-0.613739</td>\n",
       "      <td>-2.478127</td>\n",
       "      <td>-0.912046</td>\n",
       "      <td>1.931055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.471226</td>\n",
       "      <td>-1.694664</td>\n",
       "      <td>-0.049705</td>\n",
       "      <td>1.635398</td>\n",
       "      <td>4.501224</td>\n",
       "      <td>-1.859931</td>\n",
       "      <td>1.256131</td>\n",
       "      <td>3.068922</td>\n",
       "      <td>0.013728</td>\n",
       "      <td>0.779192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.992419</td>\n",
       "      <td>-0.808740</td>\n",
       "      <td>2.917089</td>\n",
       "      <td>-3.761585</td>\n",
       "      <td>-0.365662</td>\n",
       "      <td>2.248980</td>\n",
       "      <td>0.514345</td>\n",
       "      <td>-2.134935</td>\n",
       "      <td>0.280256</td>\n",
       "      <td>0.262328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Action  Adventure  Animation  Comedy  Crime  Documentary  Drama  Family  \\\n",
       "0       0          1          0       1      0            0      1       0   \n",
       "1       1          0          0       0      1            0      1       0   \n",
       "2       0          0          0       0      0            0      1       1   \n",
       "3       0          0          0       0      0            0      1       0   \n",
       "4       0          0          0       0      1            0      1       0   \n",
       "\n",
       "   Fantasy  History  ...  img_embedding_PC_0  img_embedding_PC_1  \\\n",
       "0        0        0  ...           -3.256434           -6.489767   \n",
       "1        0        0  ...           -5.427684           -1.379301   \n",
       "2        0        0  ...            2.052491            4.428200   \n",
       "3        0        0  ...           -0.471226           -1.694664   \n",
       "4        0        0  ...           -6.992419           -0.808740   \n",
       "\n",
       "   img_embedding_PC_2  img_embedding_PC_3  img_embedding_PC_4  \\\n",
       "0           -3.349805           -1.176368            5.894304   \n",
       "1           -3.140865           -1.040555           -7.670521   \n",
       "2           -0.752544           -4.224905            0.135836   \n",
       "3           -0.049705            1.635398            4.501224   \n",
       "4            2.917089           -3.761585           -0.365662   \n",
       "\n",
       "   img_embedding_PC_5  img_embedding_PC_6  img_embedding_PC_7  \\\n",
       "0           -0.583349           -1.162693            2.124027   \n",
       "1            4.405017            1.473135           -4.142650   \n",
       "2           -6.222792           -0.613739           -2.478127   \n",
       "3           -1.859931            1.256131            3.068922   \n",
       "4            2.248980            0.514345           -2.134935   \n",
       "\n",
       "   img_embedding_PC_8  img_embedding_PC_9  \n",
       "0           -5.306920            0.080211  \n",
       "1            3.978261           -0.383656  \n",
       "2           -0.912046            1.931055  \n",
       "3            0.013728            0.779192  \n",
       "4            0.280256            0.262328  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure(figsize=(12,8))\\ncorr = preprocessed_data.corr()\\n\\nheatmap = sns.heatmap(corr, cmap=\"coolwarm\"\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "plt.figure(figsize=(12,8))\n",
    "corr = preprocessed_data.corr()\n",
    "\n",
    "heatmap = sns.heatmap(corr, cmap=\"coolwarm\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creer un data training/validation splités a partir du X1 (on garde X2 pour les vrais tests) \n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, Y1, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score computation : Root Mean Square Error\n",
    "\n",
    "def compute_rmse(predict, target):\n",
    "    return mean_squared_error(predict, target, squared=False)\n",
    "\n",
    "def compute_rmse2(predict, target):\n",
    "    if len(target.shape) == 2:\n",
    "        target = target.squeeze()\n",
    "    if len(predict.shape) == 2:\n",
    "        predict = predict.squeeze()\n",
    "    diff = target - predict\n",
    "    if len(diff.shape) == 1:\n",
    "        diff = np.expand_dims(diff, axis=-1)\n",
    "    rmse = np.sqrt(diff.T@diff / diff.shape[0])\n",
    "    return float(rmse)\n",
    "\n",
    "custom_scorer = make_scorer(compute_rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_rand_search(model_, params, scoring, X_train, y_train):\n",
    "    grid = RandomizedSearchCV(model_, params, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    print(\"grid.best_score_: \", grid.best_score_)\n",
    "    print(\"grid.best_params_: \",grid.best_params_)\n",
    "    return grid\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(model_, params, scoring, X_train, y_train):\n",
    "\n",
    "    grid = GridSearchCV(model_, params, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    print(\"grid.best_score_: \", grid.best_score_)\n",
    "    print(\"grid.best_params_: \",grid.best_params_)\n",
    "    return grid\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = mutual_info_regression(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Score: -55366574.34$\n",
      "Score: 0.42%\n",
      "\n",
      "CV RMSE Score : -59988453.18$\n",
      "CV Score : 0.32%\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_test)\n",
    "\n",
    "# Évaluez le modèle en utilisant le scoreur personnalisé\n",
    "score = custom_scorer(model, X_test, y_test)\n",
    "print(\"RMSE Score: %.2f$\" % score)\n",
    "print(\"Score: %.2f%%\" %model.score(X_test, y_test))\n",
    "\n",
    "# Évaluez le modèle en utilisant le cross_val\n",
    "rmse_score = cross_val_score(LinearRegression(), preprocessed_data, Y1.values.ravel(), cv=5, scoring=custom_scorer)\n",
    "linear_regressor_score = cross_val_score(LinearRegression(), preprocessed_data, Y1.values.ravel(), cv=5)\n",
    "print(\"\\nCV RMSE Score : %.2f$\" % np.mean(rmse_score))\n",
    "print(\"CV Score : %.2f%%\" % np.mean(linear_regressor_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALISATION ENDED : initial score of 33.74%\n",
      "SOLUTION UPGRADED : new score of 39.59%\n",
      "SOLUTION UPGRADED : new score of 39.89%\n",
      "SOLUTION UPGRADED : new score of 40.01%\n",
      "SOLUTION UPGRADED : new score of 40.06%\n",
      "SOLUTION UPGRADED : new score of 40.12%\n",
      "SOLUTION UPGRADED : new score of 40.15%\n",
      "SOLUTION UPGRADED : new score of 40.18%\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, params, score):\n",
    "        self.params = params\n",
    "        self.score = score\n",
    "\n",
    "class LocalSearch :\n",
    "    \n",
    "    def __init__(self, regressor, params):\n",
    "        self.regressor = regressor\n",
    "        self.params = params\n",
    "        self.preprocessed_dict = create_preprocessed_dict(params)\n",
    "        self.visited = []\n",
    "    \n",
    "    def clean(self, params):\n",
    "        copied = copy.deepcopy(params)\n",
    "        if copied[\"production_year_style\"] == \"no_period\":\n",
    "            copied.pop(\"n_year_period\")\n",
    "        if copied[\"studio_use_PCA\"] == False:\n",
    "            copied.pop(\"studio_PCA_dim\")\n",
    "        return copied\n",
    "    \n",
    "    def is_visited(self, params):\n",
    "        return str(self.clean(params)) in self.visited\n",
    "    \n",
    "    def visit(self, params):\n",
    "        self.visited.append(str(self.clean(params)))\n",
    "    \n",
    "    def get_solution(self, params):\n",
    "        preprocessed_data = copy.deepcopy(create_preprocessed_from_dict(self.preprocessed_dict, params))\n",
    "        # current_score = np.mean(cross_val_score(regressor, preprocessed_data, Y1, cv=5, scoring='neg_root_mean_squared_error'))\n",
    "        current_score = np.mean(cross_val_score(regressor, preprocessed_data, Y1, cv=5))\n",
    "        self.visit(params)\n",
    "        return Solution(params, current_score)\n",
    "    \n",
    "    def initiate(self):\n",
    "        current_params = {}\n",
    "        for param in self.params.keys():\n",
    "            current_params[param] = np.random.choice(self.params[param])\n",
    "        self.current_solution = self.get_solution(current_params)\n",
    "        self.best_solution = copy.deepcopy(self.current_solution)\n",
    "    \n",
    "    def transitions(self):\n",
    "        neighbors = []\n",
    "        current_params = self.current_solution.params\n",
    "        for param in self.params.keys():\n",
    "            copied = copy.deepcopy(self.params[param])\n",
    "            np.random.shuffle(copied)\n",
    "            for value in copied:\n",
    "                new_params = copy.deepcopy(current_params)\n",
    "                new_params[param] = value\n",
    "                if not self.is_visited(new_params):\n",
    "                    neighbors.append(new_params)\n",
    "                    break\n",
    "        return neighbors\n",
    "    \n",
    "    def choose(self, neighbors, n):\n",
    "        if len(neighbors) <= n :\n",
    "            return neighbors\n",
    "        return np.random.choice(neighbors, size=n, replace = False)\n",
    "    \n",
    "    def get_difference(self, other):\n",
    "        first = self.current_solution.params\n",
    "        for param in first.keys():\n",
    "            if first[param] != other[param]:\n",
    "                return param, first[param], other[param]\n",
    "        return None\n",
    "    \n",
    "    def compute(self):\n",
    "        self.initiate()\n",
    "        print(\"INITIALISATION ENDED : initial score of {:.2%}\".format(self.best_solution.score))\n",
    "        no_upgrade = 0;\n",
    "        while no_upgrade <= 30:\n",
    "            neighbors = self.transitions()\n",
    "            if len(neighbors) == 0:\n",
    "                break\n",
    "            neighbors = self.choose(neighbors, 3)\n",
    "            curr_best_sol = Solution(None, -float(\"inf\"))\n",
    "            for neighbor in neighbors :\n",
    "                solution = self.get_solution(neighbor)\n",
    "                if solution.score > curr_best_sol.score :\n",
    "                    curr_best_sol = solution\n",
    "                diff = self.get_difference(neighbor)\n",
    "                # print(\"{} : {} -> {} score {:.2%}\".format(diff[0], diff[1], diff[2],solution.score))\n",
    "            self.current_solution = copy.deepcopy(curr_best_sol)\n",
    "            # print(\"updated current\")\n",
    "            if curr_best_sol.score >= self.best_solution.score:\n",
    "                self.best_solution = copy.deepcopy(curr_best_sol)\n",
    "                print(\"SOLUTION UPGRADED : new score of {:.2%}\".format(self.best_solution.score))\n",
    "                no_upgrade = 0\n",
    "            no_upgrade += 1\n",
    "        \n",
    "params = {\n",
    "    \"production_year_style\" : [\"per_quantile\", \"per_period_length\", \"no_period\"], # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
    "    \"n_year_period\" : [3, 5, 10],\n",
    "    \"runtime_replace_type\" : [\"mean\", \"zero\"], # \"mean\" / \"zero\"\n",
    "    \"studio_use_PCA\" : [False, True],\n",
    "    \"studio_PCA_dim\" : [1, 10],\n",
    "    \"text_embedding_PCA_dim\" : [1, 10, 20, 50, 100],\n",
    "    \"img_embedding_PCA_dim\" : [1, 10, 20, 50, 100]\n",
    "}\n",
    "\n",
    "regressor = LinearRegression(copy_X = True, n_jobs=-1)\n",
    "localsearch = LocalSearch(regressor, params)\n",
    "localsearch.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40184168966812434\n",
      "{'production_year_style': 'per_quantile', 'n_year_period': 10, 'runtime_replace_type': 'zero', 'studio_use_PCA': False, 'studio_PCA_dim': 1, 'text_embedding_PCA_dim': 10, 'img_embedding_PCA_dim': 1}\n"
     ]
    }
   ],
   "source": [
    "print(localsearch.best_solution.score)\n",
    "print(localsearch.best_solution.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch on RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid.best_score_:  -73260650.02901699\n",
      "grid.best_params_:  {'criterion': 'squared_error', 'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 1.0, 'n_estimators': 500}\n",
      "\n",
      "RMSE Score: -72645305.75$\n",
      "Score: -0.00%\n",
      "\n",
      "CV RMSE Score : -73005230.30$\n",
      "CV Score : -0.00%\n"
     ]
    }
   ],
   "source": [
    "## GridSearch\n",
    "# Définir les valeurs à tester pour les hyperparamètres du modèle\n",
    "params =    {\"n_estimators\": [500], \n",
    "            \"criterion\": [\"squared_error\"], # [\"squared_error\", \"absolute_error\", \"poisson\"],\n",
    "            \"max_depth\": [1],               # [None, 1, 2, 3],\n",
    "            \"min_samples_split\": [1.0],       # [1, 2, 3],\n",
    "            \"max_features\" : [\"sqrt\"],      # [\"sqrt\", \"log2\", None]\n",
    "            }\n",
    "rfr = RandomForestRegressor(n_jobs=-1,random_state=0)\n",
    "grid = perform_grid_search(rfr, params, custom_scorer, X_train, y_train)\n",
    "\n",
    "\n",
    "model = RandomForestRegressor(n_estimators = 500, \n",
    "                                criterion = \"squared_error\", # [\"squared_error\", \"absolute_error\", \"poisson\"],\n",
    "                                max_depth = 1,               # [None, 1, 2, 3],\n",
    "                                min_samples_split = 1.0,       # [1, 2, 3],\n",
    "                                max_features = \"sqrt\",      # [\"sqrt\", \"log2\", None]\n",
    "                                n_jobs=-1,\n",
    "                                random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_test)\n",
    "\n",
    "# Évaluez le modèle en utilisant le scoreur personnalisé\n",
    "score = custom_scorer(model, X_test, y_test)\n",
    "print(\"\\nRMSE Score: %.2f$\" % score)\n",
    "print(\"Score: %.2f%%\" %model.score(X_test, y_test))\n",
    "\n",
    "# Évaluez le modèle en utilisant le cross_val\n",
    "rmse_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5,scoring=custom_scorer)\n",
    "cv_regressor_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5)\n",
    "print(\"\\nCV RMSE Score : %.2f$\" % np.mean(rmse_score))\n",
    "print(\"CV Score : %.2f%%\" % np.mean(cv_regressor_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch on KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m    {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_neighbors\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m), \n\u001b[0;32m      4\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m : [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[0;32m      5\u001b[0m knn \u001b[38;5;241m=\u001b[39m KNeighborsRegressor(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[43mperform_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_scorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# CHECK LES RESULTATS\u001b[39;00m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m KNeighborsRegressor(n_neighbors\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m43\u001b[39m, weights\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36mperform_grid_search\u001b[1;34m(model_, params, scoring, X_train, y_train)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_grid_search\u001b[39m(model_, params, scoring, X_train, y_train):\n\u001b[0;32m      3\u001b[0m     grid \u001b[38;5;241m=\u001b[39m GridSearchCV(model_, params, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39mscoring, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid.best_score_: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid\u001b[38;5;241m.\u001b[39mbest_score_)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrid.best_params_: \u001b[39m\u001b[38;5;124m\"\u001b[39m,grid\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m     )\n\u001b[1;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    860\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## GridSearch\n",
    "# Définir les valeurs à tester pour les hyperparamètres du modèle\n",
    "params =    {'n_neighbors' : range(1, 50), \n",
    "            'weights' : ['uniform', 'distance']}\n",
    "knn = KNeighborsRegressor(n_jobs=-1)\n",
    "grid = perform_grid_search(knn, params, custom_scorer, X_train, y_train)\n",
    "\n",
    "# CHECK LES RESULTATS\n",
    "model = KNeighborsRegressor(n_neighbors= 43, weights= 'uniform', n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_test)\n",
    "\n",
    "# Évaluez le modèle en utilisant le scoreur personnalisé\n",
    "score = custom_scorer(model, X_test, y_test)\n",
    "print(\"RMSE Score: %.2f$\" % score)\n",
    "print(\"Score: %.2f%%\" %model.score(X_test, y_test))\n",
    "\n",
    "# Évaluez le modèle en utilisant le cross_val\n",
    "rmse_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5,scoring=custom_scorer)\n",
    "knn_regressor_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5)\n",
    "print(\"\\nCV RMSE Score : %.2f$\" % np.mean(rmse_score))\n",
    "print(\"CV Score : %.2f%%\" % np.mean(knn_regressor_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid on MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "params =    {'hidden_layer_sizes' : [(50,50,50), (100,5), (100,)],\n",
    "            'activation' : ['relu'],\n",
    "            'solver': ['adam'],\n",
    "            'alpha' : np.linspace(0.0001, 0.001, 10, endpoint = True),\n",
    "            'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "            'max_iter': [500]}\n",
    "model = MLPRegressor(random_state=0)\n",
    "\n",
    "grid = perform_grid_search(model, params, custom_scorer, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(activation = 'relu', \n",
    "                    alpha = 0.0007, \n",
    "                    hidden_layer_sizes = (50, 50, 50), \n",
    "                    learning_rate_init = 0.1, \n",
    "                    max_iter = 500, \n",
    "                    solver = 'adam',\n",
    "                    random_state=0)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_test)\n",
    "\n",
    "# Évaluez le modèle en utilisant le scoreur personnalisé\n",
    "score = custom_scorer(model, X_test, y_test)\n",
    "print(\"RMSE Score: %.2f$\" % score)\n",
    "print(\"Score: %.2f%%\" % model.score(X_test, y_test))\n",
    "\n",
    "# Évaluez le modèle en utilisant le cross_val\n",
    "rmse_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5,scoring=custom_scorer)\n",
    "mlp_regressor_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5)\n",
    "print(\"\\nCV RMSE Score : %.2f$\" % np.mean(rmse_score))\n",
    "print(\"CV Score : %.2f%%\" % np.mean(mlp_regressor_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for each training subset\n",
    "N, train_score, val_score = learning_curve(model, X_train, y_train, cv=5)\n",
    "\n",
    "plt.plot(N, train_score.mean(axis=1), label='train')\n",
    "plt.plot(N, val_score.mean(axis=1), label='validation')\n",
    "plt.xlabel(\"train size\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics from the gridsearch\n",
    "res = pd.DataFrame(grid.cv_results_.values(), ).transpose()\n",
    "res.columns=grid.cv_results_.keys()\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9abed8abd2ce500fe15b078df5e406127fe6c9e241ddd0cff9b21749bf3b9f14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
