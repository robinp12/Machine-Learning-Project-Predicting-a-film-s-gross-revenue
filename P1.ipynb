{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPQ-V5gypUhp"
      },
      "source": [
        "## Import datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EcJ5uOstpUhs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "from scipy import stats\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AhsvRFo0pUhu"
      },
      "outputs": [],
      "source": [
        "# Use pandas to loadinto a DataFrame\n",
        "# Y1.csv doesn’t have a header so\n",
        "# add one when loading the file\n",
        "X1 = pd.read_csv(\"data/X1.csv\")\n",
        "Y1 = pd.read_csv(\"data/Y1.csv\", header=None, names=['revenue'])\n",
        "\n",
        "\n",
        "# ENLEVER colonne \"Unnamed\" du dataset : utilité ? -> Pas listé dans les features du pdf\n",
        "X = X1.drop(['Unnamed: 0', 'img_url', 'description'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhlqrDoSpUhv"
      },
      "source": [
        "### Pre data visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GhjHfT_xpUhv",
        "outputId": "e0589568-b214-46df-891a-48d896fb1d33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(X.describe())\\n\\nplt.figure(figsize=(7,4))\\ncorr = X.corr(numeric_only=True)\\nheatmap = sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\\'.1g\\')\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\"\"\"\n",
        "print(X.describe())\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "corr = X.corr(numeric_only=True)\n",
        "heatmap = sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt='.1g')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl3yWOPPpUhw"
      },
      "source": [
        "## Preprocessing the data\n",
        "### Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TNdqOum4pUhw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Creating empty DataFrame to start\n",
        "\"\"\"\n",
        "n_samples = X.shape[0]\n",
        "data = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ce4OZO08pUhw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Keeping the directly usable features\n",
        "\"\"\"\n",
        "def get_directrly_usable_features(df):\n",
        "    directly_usable_features = [\"ratings\", \"n_votes\", \"is_adult\"]\n",
        "    for feature in directly_usable_features:\n",
        "        df[feature] = X[feature]\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "scrolled": false,
        "id": "0YGTbsy5pUhw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dealing with the \"production_year\" feature\n",
        "\"\"\"\n",
        "\n",
        "def get_prod_year_feature(df, params):\n",
        "    \n",
        "    style = params[\"production_year_style\"] # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
        "    if style != \"no_period\" :\n",
        "        n_year_period = params[\"n_year_period\"]\n",
        "\n",
        "    # Removing previously computed categorie(s) for the \"production_year\" initial feature\n",
        "\n",
        "    for feature in df.columns:\n",
        "        if len(feature) >= 8 and (feature[:6] == \"period\" or feature == \"production_year\"):\n",
        "            df.drop(feature, axis=1, inplace=True)\n",
        "\n",
        "    # Creating new categorie(s) for the \"production_year\" initial feature\n",
        "\n",
        "    prod_year = X[\"production_year\"].copy()\n",
        "    if style == \"per_quantile\" or style == \"per_period_length\":\n",
        "        categories = np.ones((n_year_period, n_samples))\n",
        "\n",
        "        if style == \"per_quantile\":\n",
        "            thresholds = prod_year.quantile(np.arange(1, n_year_period) / n_year_period)\n",
        "        else :\n",
        "            thresholds = np.min(prod_year) + (np.max(prod_year) - np.min(prod_year))*np.arange(1, n_year_period)/n_year_period\n",
        "        for i, threshold in enumerate(thresholds):\n",
        "            categories[i+1] = (prod_year >= threshold).astype(int)\n",
        "            categories[i] -= categories[i+1]\n",
        "        for period in range(n_year_period):\n",
        "            df[\"period {}\".format(period)] = categories[period]\n",
        "    elif style == \"no_period\":\n",
        "        df[\"production_year\"] = prod_year\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "izRYAX6LpUhx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dealing with the \"runtime\" feature\n",
        "\n",
        "The problem is here that we have some missing values, we have to deal with it.\n",
        "\"\"\"\n",
        "\n",
        "def get_runtime_feature(df, params):\n",
        "\n",
        "    # Add other smarter ways ?\n",
        "    \n",
        "    replace_type = params[\"runtime_replace_type\"] # \"zero\" / \"mean\"\n",
        "    \n",
        "    runtime = X[\"runtime\"].copy()\n",
        "    if replace_type == \"zero\":\n",
        "        runtime[runtime == \"\\\\N\"] = 0\n",
        "    if replace_type == \"mean\":\n",
        "        mean = np.mean(runtime[runtime != \"\\\\N\"].astype(float))\n",
        "        runtime[runtime == \"\\\\N\"] = mean\n",
        "    df[\"runtime\"] = runtime.astype(float)    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8xuppfx9pUhy"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\"\"\"\n",
        "Dealing with the \"studio\" feature\n",
        "\n",
        "\n",
        "Juste rajouter toute les features une par une me semblait un peu lourd (yen a 509), ducoup j'effectue PCA dessus.\n",
        "Jsp si ça se fait ? (on peut changer l'algo de dimensionality reduction aussi si on veut)\n",
        "\"\"\"\n",
        "\n",
        "def get_studio_feature(df, params):\n",
        "    use_PCA = params[\"studio_use_PCA\"]\n",
        "    if use_PCA :\n",
        "        dim = params[\"studio_PCA_dim\"]\n",
        "\n",
        "    # Removing previously computed categorie(s) for the \"studio\" initial feature\n",
        "    for feature in df.columns:\n",
        "        if len(feature) >= 10 and feature[:10] == \"studio_PC_\":\n",
        "            df.drop(feature, axis=1, inplace=True)\n",
        "\n",
        "    # Creating new categorie(s) for the \"studio\" initial feature\n",
        "    studio = copy.deepcopy(X[\"studio\"])\n",
        "    studio_labels = np.unique(studio)\n",
        "    studio_features = np.zeros((len(studio_labels), n_samples))\n",
        "    for i, label in enumerate(studio_labels) :\n",
        "        studio_features[i] = (studio == label).astype(int)\n",
        "    \n",
        "    # Applying pca or not\n",
        "    if use_PCA :\n",
        "        pca = PCA(n_components=dim)\n",
        "        out = pca.fit_transform(studio_features.T)\n",
        "    else :\n",
        "        normals = studio_features[np.count_nonzero(studio_features, axis=1) > 5].T\n",
        "        outliers = np.sum(studio_features[np.count_nonzero(studio_features, axis=1) <= 5].T, axis = 1)\n",
        "        out = np.zeros((normals.shape[0], normals.shape[1] + 1))\n",
        "        out[:, :-1] = normals\n",
        "        out[:, -1] = outliers\n",
        "        dim = out.shape[1]\n",
        "    \n",
        "    df[[\"studio_PC_{}\".format(i) for i in range(dim)]] = out\n",
        "    return df\n",
        "\n",
        "# Ya plein de warnings quand dim trop grand ou pas de PCA /: \n",
        "# jsp comment regler ça... en utilisant pd.concat ça tourne vraiment extrêmement lentement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mqsGYb9ApUhz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dealing with the \"genres\" feature\n",
        "\n",
        "Je rajoute juste une feature par genre, j'espère ça suffit ? -> pt-être dimension reduction sur ça aussi ?\n",
        "Certains films ont pas de genre attitré (genre = \"\\\\N\"),\n",
        "ce que j'ai fait ici considère ça comme un genre à part entière, ptet on peut deal le truc autrement ?\n",
        "\"\"\"\n",
        "\n",
        "def get_genre_feature(df):\n",
        "\n",
        "    X.loc[X[\"genres\"] == \"\\\\N\", \"genres\"] = \"Others\"\n",
        "    all_genres = X[\"genres\"].copy()\n",
        "    diff_genres = []\n",
        "\n",
        "    for genres in np.unique(all_genres):\n",
        "        for genre in genres.split(\",\") :\n",
        "            if not genre in diff_genres :\n",
        "                diff_genres.append(genre)\n",
        "\n",
        "    for genre in diff_genres:\n",
        "        df[genre] = [1 if genre in genres.split(\",\") else 0 for genres in all_genres]  \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "4APQi5pKpUhz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dealing with the \"text_embeddings\" feature\n",
        "\n",
        "\n",
        "Dimension of embedding space is too high -> dimensionnality reduction\n",
        "J'utilise que PCA pr l'instant ici aussi\n",
        "\"\"\"\n",
        "def get_text_embedding_feature(df, params):\n",
        "    \n",
        "    output_dim = params[\"text_embedding_PCA_dim\"] # output dimension of PCA\n",
        "\n",
        "    # Removing previously computed categorie(s) for the \"text_embedding\" initial feature\n",
        "    for feature in df.columns:\n",
        "        if len(feature) >= 18 and feature[:18] == \"text_embedding_PC_\":\n",
        "            df.drop(feature, axis=1, inplace=True)\n",
        "\n",
        "    # Creating new categorie(s) for the \"text_embedding\" initial feature\n",
        "    text_embeddings = X[\"text_embeddings\"]\n",
        "    input_dim = 768\n",
        "    embeddings = np.zeros((n_samples, input_dim))\n",
        "    for i, text_embedding in enumerate(text_embeddings):\n",
        "        embeddings[i] = list(map(float,text_embedding[1:-1].split(\",\")))\n",
        "\n",
        "    # applying PCA\n",
        "    pca = PCA(n_components=output_dim)\n",
        "    output = pca.fit_transform(embeddings)\n",
        "\n",
        "    df[[\"text_embedding_PC_{}\".format(i) for i in range(output_dim)]] = output\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Lrg6Ft1zpUh0"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dealing with the \"img_embeddings\" feature\n",
        "\n",
        "\n",
        "Dimension of embedding space is too high -> dimensionnality reduction\n",
        "J'utilise que PCA pr l'instant ici aussi\n",
        "\"\"\"\n",
        "def get_img_embedding_feature(df, params):\n",
        "    output_dim = params[\"img_embedding_PCA_dim\"] # output dimension of PCA\n",
        "\n",
        "    # Removing previously computed categorie(s) for the \"img_embedding\" initial feature\n",
        "    for feature in df.columns:\n",
        "        if len(feature) >= 17 and feature[:17] == \"img_embedding_PC_\":\n",
        "            df.drop(feature, axis=1, inplace=True)\n",
        "\n",
        "    # Creating new categorie(s) for the \"img_embedding\" initial feature\n",
        "    img_embeddings = X[\"img_embeddings\"]\n",
        "    input_dim = 2048\n",
        "    embeddings = np.zeros((n_samples, input_dim))\n",
        "    for i, img_embedding in enumerate(img_embeddings):\n",
        "        embeddings[i] = list(map(float,img_embedding[1:-1].split(\",\")))\n",
        "\n",
        "    # applying PCA\n",
        "    pca = PCA(n_components=output_dim)\n",
        "    output = pca.fit_transform(embeddings)\n",
        "\n",
        "    df[[\"img_embedding_PC_{}\".format(i) for i in range(output_dim)]] = output\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "a0DOzTyBpUh0"
      },
      "outputs": [],
      "source": [
        "def create_preprocessed(params):\n",
        "    df = pd.DataFrame()\n",
        "    df = get_genre_feature(df)\n",
        "    df = get_directrly_usable_features(df)\n",
        "    df = get_prod_year_feature(df, params)\n",
        "    df = get_studio_feature(df, params)\n",
        "    df = get_runtime_feature(df, params)\n",
        "    df = get_text_embedding_feature(df, params)\n",
        "    df = get_img_embedding_feature(df, params)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bzhpiUa1pUh1"
      },
      "outputs": [],
      "source": [
        "def create_preprocessed_dict(params):\n",
        "    final = {}\n",
        "    year_dict = {}\n",
        "    for val1 in params[\"production_year_style\"] :\n",
        "        for val2 in params[\"n_year_period\"] :\n",
        "            tmp = {}\n",
        "            tmp[\"production_year_style\"] = val1\n",
        "            tmp[\"n_year_period\"] = val2\n",
        "            year_dict[val1+str(val2)] = get_prod_year_feature(pd.DataFrame(), tmp)\n",
        "    final[\"year\"] = year_dict\n",
        "    \n",
        "    studio_dict = {}\n",
        "    for val1 in params[\"studio_use_PCA\"] :\n",
        "        for val2 in params[\"studio_PCA_dim\"] :\n",
        "            tmp = {}\n",
        "            tmp[\"studio_use_PCA\"] = val1\n",
        "            tmp[\"studio_PCA_dim\"] = val2\n",
        "            studio_dict[str(val1)+str(val2)] = get_studio_feature(pd.DataFrame(), tmp)\n",
        "    final[\"studio\"] = studio_dict\n",
        "    \n",
        "    runtime_dict = {}\n",
        "    for val in params[\"runtime_replace_type\"] :\n",
        "        tmp = {}\n",
        "        tmp[\"runtime_replace_type\"] = val\n",
        "        runtime_dict[val] = get_runtime_feature(pd.DataFrame(), tmp)\n",
        "    final[\"runtime\"] = runtime_dict\n",
        "    \n",
        "    text_dict = {}\n",
        "    for val in params[\"text_embedding_PCA_dim\"] :\n",
        "        tmp = {}\n",
        "        tmp[\"text_embedding_PCA_dim\"] = val\n",
        "        text_dict[val] = get_text_embedding_feature(pd.DataFrame(), tmp)\n",
        "    final[\"text\"] = text_dict\n",
        "    \n",
        "    img_dict = {}\n",
        "    for val in params[\"img_embedding_PCA_dim\"] :\n",
        "        tmp = {}\n",
        "        tmp[\"img_embedding_PCA_dim\"] = val\n",
        "        img_dict[val] = get_img_embedding_feature(pd.DataFrame(), tmp)\n",
        "    final[\"img\"] = img_dict   \n",
        "    final[\"genre\"] = get_genre_feature(pd.DataFrame())\n",
        "    final[\"base\"] = get_directrly_usable_features(pd.DataFrame())\n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kUl_tHK0pUh1"
      },
      "outputs": [],
      "source": [
        "def create_preprocessed_from_dict(params_dict, params):\n",
        "    frames = []\n",
        "    frames.append(params_dict[\"genre\"])\n",
        "    frames.append(params_dict[\"base\"])\n",
        "    frames.append(params_dict[\"year\"][params[\"production_year_style\"] + str(params[\"n_year_period\"])])\n",
        "    frames.append(params_dict[\"studio\"][str(params[\"studio_use_PCA\"]) + str(params[\"studio_PCA_dim\"])])\n",
        "    frames.append(params_dict[\"runtime\"][params[\"runtime_replace_type\"]])\n",
        "    frames.append(params_dict[\"text\"][params[\"text_embedding_PCA_dim\"]])\n",
        "    frames.append(params_dict[\"img\"][params[\"img_embedding_PCA_dim\"]])\n",
        "    return pd.concat(frames, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzu12HJOpUh2"
      },
      "source": [
        "### Preprocessing execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "L6CQd5jzpUh2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Example of data preprocessing\n",
        "\"\"\"\n",
        "\n",
        "params = {\n",
        "    \"production_year_style\" : \"per_quantile\", # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
        "    \"n_year_period\" : 5,\n",
        "    \"runtime_replace_type\" : \"mean\", # \"mean\" / \"zero\"\n",
        "    \"studio_use_PCA\" : True,\n",
        "    \"studio_PCA_dim\" : 50,\n",
        "    \"text_embedding_PCA_dim\" : 50,\n",
        "    \"img_embedding_PCA_dim\" : 50\n",
        "}\n",
        "\n",
        "preprocessed_data = create_preprocessed(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "uAyDWmdupUh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d024fbf7-768d-4b1b-f8db-af4f93c6bf47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nall_params = {\\n    \"production_year_style\" : [\"per_quantile\", \"per_period_length\", \"no_period\"], # \"per_quantile\" / \"per_period_length\" / \"no_period\"\\n    \"n_year_period\" : [3, 5, 10],\\n    \"runtime_replace_type\" : [\"mean\", \"zero\"], # \"mean\" / \"zero\"\\n    \"studio_use_PCA\" : [True],\\n    \"studio_PCA_dim\" : [1, 10],\\n    \"text_embedding_PCA_dim\" : [1, 10],\\n    \"img_embedding_PCA_dim\" : [1, 10]\\n}\\n\\nparams = {\\n    \"production_year_style\" : \"per_quantile\", # \"per_quantile\" / \"per_period_length\" / \"no_period\"\\n    \"n_year_period\" : 5,\\n    \"runtime_replace_type\" : \"mean\", # \"mean\" / \"zero\"\\n    \"studio_use_PCA\" : True,\\n    \"studio_PCA_dim\" : 1,\\n    \"text_embedding_PCA_dim\" : 10,\\n    \"img_embedding_PCA_dim\" : 10\\n}\\n\\npreprocessed_dict = create_preprocessed_dict(all_params)\\npreprocessed_data = create_preprocessed_from_dict(preprocessed_dict, params)\\npreprocessed_data_2 = create_preprocessed(params)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "\"\"\"\n",
        "all_params = {\n",
        "    \"production_year_style\" : [\"per_quantile\", \"per_period_length\", \"no_period\"], # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
        "    \"n_year_period\" : [3, 5, 10],\n",
        "    \"runtime_replace_type\" : [\"mean\", \"zero\"], # \"mean\" / \"zero\"\n",
        "    \"studio_use_PCA\" : [True],\n",
        "    \"studio_PCA_dim\" : [1, 10],\n",
        "    \"text_embedding_PCA_dim\" : [1, 10],\n",
        "    \"img_embedding_PCA_dim\" : [1, 10]\n",
        "}\n",
        "\n",
        "params = {\n",
        "    \"production_year_style\" : \"per_quantile\", # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
        "    \"n_year_period\" : 5,\n",
        "    \"runtime_replace_type\" : \"mean\", # \"mean\" / \"zero\"\n",
        "    \"studio_use_PCA\" : True,\n",
        "    \"studio_PCA_dim\" : 1,\n",
        "    \"text_embedding_PCA_dim\" : 10,\n",
        "    \"img_embedding_PCA_dim\" : 10\n",
        "}\n",
        "\n",
        "preprocessed_dict = create_preprocessed_dict(all_params)\n",
        "preprocessed_data = create_preprocessed_from_dict(preprocessed_dict, params)\n",
        "preprocessed_data_2 = create_preprocessed(params)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OoIB-BycpUh4"
      },
      "outputs": [],
      "source": [
        "# preprocessed_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vZeeGjnQpUh4"
      },
      "outputs": [],
      "source": [
        "# preprocessed_data_2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHiFHOO6pUh4"
      },
      "source": [
        "### Data visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "filTNaiOpUh5",
        "outputId": "099b14e1-3c49-4a56-bf20-888355bd567e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nplt.figure(figsize=(12,8))\\ncorr = preprocessed_data.corr()\\n\\nheatmap = sns.heatmap(corr, cmap=\"coolwarm\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "\"\"\"\n",
        "plt.figure(figsize=(12,8))\n",
        "corr = preprocessed_data.corr()\n",
        "\n",
        "heatmap = sns.heatmap(corr, cmap=\"coolwarm\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvaQgGkDpUh5"
      },
      "source": [
        "### Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nYz1_ruOpUh5"
      },
      "outputs": [],
      "source": [
        "# Creer un data training/validation splités a partir du X1 (on garde X2 pour les vrais tests) \n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, Y1, test_size = 0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "XJBIIkxRpUh5"
      },
      "outputs": [],
      "source": [
        "# Score computation : Root Mean Square Error\n",
        "\n",
        "def compute_rmse(predict, target):\n",
        "    return -mean_squared_error(predict, target, squared=False)\n",
        "\n",
        "def compute_rmse2(predict, target):\n",
        "    if len(target.shape) == 2:\n",
        "        target = target.squeeze()\n",
        "    if len(predict.shape) == 2:\n",
        "        predict = predict.squeeze()\n",
        "    diff = target - predict\n",
        "    if len(diff.shape) == 1:\n",
        "        diff = np.expand_dims(diff, axis=-1)\n",
        "    rmse = np.sqrt(diff.T@diff / diff.shape[0])\n",
        "    return float(rmse)\n",
        "\n",
        "custom_scorer = make_scorer(compute_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "sZs7SNtApUh6"
      },
      "outputs": [],
      "source": [
        "def perform_rand_search(model_, params, scoring, X_train, y_train):\n",
        "    grid = RandomizedSearchCV(model_, params, cv=5, scoring=scoring, n_jobs=-1)\n",
        "    grid.fit(X_train,y_train)\n",
        "    print(\"grid.best_score_: \", grid.best_score_)\n",
        "    print(\"grid.best_params_: \",grid.best_params_)\n",
        "    return grid\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "TSg1clavpUh6"
      },
      "outputs": [],
      "source": [
        "def perform_grid_search(model_, params, scoring, X_train, y_train):\n",
        "\n",
        "    grid = GridSearchCV(model_, params, cv=5, scoring=scoring, n_jobs=-1)\n",
        "    grid.fit(X_train,y_train)\n",
        "    print(\"grid.best_score_: \", grid.best_score_)\n",
        "    print(\"grid.best_params_: \",grid.best_params_)\n",
        "    return grid\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ARFzUQaOpUh6"
      },
      "outputs": [],
      "source": [
        "scores = mutual_info_regression(X_train, np.ravel(y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmVfJDSIpUh6"
      },
      "source": [
        "### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoDVBuslpUh7",
        "outputId": "e4a288bd-c0c1-4129-a87f-92363b9eeab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE Score: -52994656.12$\n",
            "Score: 0.47%\n",
            "\n",
            "CV RMSE Score : -56975223.25$\n",
            "CV Score : 0.39%\n"
          ]
        }
      ],
      "source": [
        "model = LinearRegression(n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "model.predict(X_test)\n",
        "\n",
        "# Évaluez le modèle en utilisant le scoreur personnalisé\n",
        "score = custom_scorer(model, X_test, y_test)\n",
        "print(\"RMSE Score: %.2f$\" % score)\n",
        "print(\"Score: %.2f%%\" %model.score(X_test, y_test))\n",
        "\n",
        "# Évaluez le modèle en utilisant le cross_val\n",
        "rmse_score = cross_val_score(LinearRegression(), preprocessed_data, Y1.values.ravel(), cv=5, scoring=custom_scorer)\n",
        "linear_regressor_score = cross_val_score(LinearRegression(), preprocessed_data, Y1.values.ravel(), cv=5)\n",
        "print(\"\\nCV RMSE Score : %.2f$\" % np.mean(rmse_score))\n",
        "print(\"CV Score : %.2f%%\" % np.mean(linear_regressor_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kRdsahtpUh7",
        "outputId": "9a9cbdd3-91f2-4f4f-87e3-3e2336c4bf94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INITIALISATION ENDED : initial score of -5.80e+07 $\n",
            "SOLUTION UPGRADED : new score of -5.80e+07 $\n",
            "SOLUTION UPGRADED : new score of -5.75e+07 $\n",
            "SOLUTION UPGRADED : new score of -5.69e+07 $\n",
            "SOLUTION UPGRADED : new score of -5.66e+07 $\n",
            "SOLUTION UPGRADED : new score of -5.65e+07 $\n",
            "SOLUTION UPGRADED : new score of -5.65e+07 $\n",
            "SOLUTION UPGRADED : new score of -5.65e+07 $\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "class Solution:\n",
        "    def __init__(self, params, score):\n",
        "        self.params = params\n",
        "        self.score = score\n",
        "\n",
        "class LocalSearch :\n",
        "    \n",
        "    def __init__(self, regressor, params):\n",
        "        self.regressor = regressor\n",
        "        self.params = params\n",
        "        self.preprocessed_dict = create_preprocessed_dict(params)\n",
        "        self.visited = []\n",
        "\n",
        "    \n",
        "    def clean(self, params):\n",
        "        copied = copy.deepcopy(params)\n",
        "        if copied[\"production_year_style\"] == \"no_period\":\n",
        "            copied.pop(\"n_year_period\")\n",
        "        if copied[\"studio_use_PCA\"] == False:\n",
        "            copied.pop(\"studio_PCA_dim\")\n",
        "        return copied\n",
        "    \n",
        "    def is_visited(self, params):\n",
        "        return str(self.clean(params)) in self.visited\n",
        "    \n",
        "    def visit(self, params):\n",
        "        self.visited.append(str(self.clean(params)))\n",
        "    \n",
        "    def get_solution(self, params):\n",
        "        preprocessed_data = copy.deepcopy(create_preprocessed_from_dict(self.preprocessed_dict, params))\n",
        "\n",
        "        current_score = np.mean(cross_val_score(regressor, preprocessed_data, Y1, cv=5, scoring=custom_scorer))\n",
        "        \n",
        "\n",
        "        # current_score = np.mean(cross_val_score(regressor, preprocessed_data, Y1, cv=5))\n",
        "        self.visit(params)\n",
        "        return Solution(params, current_score)\n",
        "    \n",
        "    def initiate(self):\n",
        "        current_params = {}\n",
        "        for param in self.params.keys():\n",
        "            current_params[param] = np.random.choice(self.params[param])\n",
        "        self.current_solution = self.get_solution(current_params)\n",
        "        self.best_solution = copy.deepcopy(self.current_solution)\n",
        "    \n",
        "    def transitions(self):\n",
        "        neighbors = []\n",
        "        current_params = self.current_solution.params\n",
        "        for param in self.params.keys():\n",
        "            copied = copy.deepcopy(self.params[param])\n",
        "            np.random.shuffle(copied)\n",
        "            for value in copied:\n",
        "                new_params = copy.deepcopy(current_params)\n",
        "                new_params[param] = value\n",
        "                if not self.is_visited(new_params):\n",
        "                    neighbors.append(new_params)\n",
        "                    break\n",
        "        return neighbors\n",
        "    \n",
        "    def choose(self, neighbors, n):\n",
        "        if len(neighbors) <= n :\n",
        "            return neighbors\n",
        "        return np.random.choice(neighbors, size=n, replace = False)\n",
        "    \n",
        "    def get_difference(self, other):\n",
        "        first = self.current_solution.params\n",
        "        for param in first.keys():\n",
        "            if first[param] != other[param]:\n",
        "                return param, first[param], other[param]\n",
        "        return None\n",
        "    \n",
        "    def compute(self):\n",
        "        self.initiate()\n",
        "        print(\"INITIALISATION ENDED : initial score of {:.2e} $\".format(self.best_solution.score))\n",
        "        no_upgrade = 0;\n",
        "        while no_upgrade <= 30:\n",
        "            neighbors = self.transitions()\n",
        "            if len(neighbors) == 0:\n",
        "                break\n",
        "            neighbors = self.choose(neighbors, 3)\n",
        "            curr_best_sol = Solution(None, -float(\"inf\"))\n",
        "            for neighbor in neighbors :\n",
        "                solution = self.get_solution(neighbor)\n",
        "                if solution.score > curr_best_sol.score :\n",
        "                    curr_best_sol = solution\n",
        "                diff = self.get_difference(neighbor)\n",
        "                # print(\"{} : {} -> {} score {:.2%}\".format(diff[0], diff[1], diff[2],solution.score))\n",
        "            self.current_solution = copy.deepcopy(curr_best_sol)\n",
        "            # print(\"updated current\")\n",
        "            if curr_best_sol.score >= self.best_solution.score:\n",
        "                self.best_solution = copy.deepcopy(curr_best_sol)\n",
        "                print(\"SOLUTION UPGRADED : new score of {:.2e} $\".format(self.best_solution.score))\n",
        "                no_upgrade = 0\n",
        "            no_upgrade += 1\n",
        "        \n",
        "params = {\n",
        "    \"production_year_style\" : [\"per_quantile\", \"per_period_length\", \"no_period\"], # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
        "    \"n_year_period\" : [3, 5, 10],\n",
        "    \"runtime_replace_type\" : [\"mean\", \"zero\"], # \"mean\" / \"zero\"\n",
        "    \"studio_use_PCA\" : [False, True],\n",
        "    \"studio_PCA_dim\" : [1, 10],\n",
        "    \"text_embedding_PCA_dim\" : [1, 10, 20, 50, 100],\n",
        "    \"img_embedding_PCA_dim\" : [1, 10, 20, 50, 100]\n",
        "}\n",
        "\n",
        "regressor = MLPRegressor(activation = 'relu', \n",
        "            alpha = 0.0007, \n",
        "            hidden_layer_sizes = (50, 50, 50), \n",
        "            learning_rate_init = 0.1, \n",
        "            max_iter = 500, \n",
        "            solver = 'adam',\n",
        "            random_state=0)\n",
        "regressor = LinearRegression()\n",
        "\n",
        "localsearch = LocalSearch(regressor, params)\n",
        "localsearch.compute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsShyRnFpUh7",
        "outputId": "09d8f68a-3ed3-4edb-d321-a794a8a3d1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-56480647.43792944\n",
            "{'production_year_style': 'per_quantile', 'n_year_period': 10, 'runtime_replace_type': 'zero', 'studio_use_PCA': False, 'studio_PCA_dim': 10, 'text_embedding_PCA_dim': 10, 'img_embedding_PCA_dim': 1}\n"
          ]
        }
      ],
      "source": [
        "print(localsearch.best_solution.score)\n",
        "print(localsearch.best_solution.params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR_-tyXRpUh8"
      },
      "source": [
        "### GridSearch on RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBh0uEUapUh8",
        "outputId": "8954f34e-3e2f-44cc-eb01-3b92c093525b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid.best_score_:  -73260650.02901699\n",
            "grid.best_params_:  {'criterion': 'squared_error', 'max_depth': 1, 'max_features': 'sqrt', 'min_samples_split': 1.0, 'n_estimators': 500}\n",
            "\n",
            "RMSE Score: -72645305.75$\n",
            "Score: -0.00%\n",
            "\n",
            "CV RMSE Score : -73005230.30$\n",
            "CV Score : -0.00%\n"
          ]
        }
      ],
      "source": [
        "## GridSearch\n",
        "# Définir les valeurs à tester pour les hyperparamètres du modèle\n",
        "params =    {\"n_estimators\": [500], \n",
        "            \"criterion\": [\"squared_error\"], # [\"squared_error\", \"absolute_error\", \"poisson\"],\n",
        "            \"max_depth\": [1],               # [None, 1, 2, 3],\n",
        "            \"min_samples_split\": [1.0],       # [1, 2, 3],\n",
        "            \"max_features\" : [\"sqrt\"],      # [\"sqrt\", \"log2\", None]\n",
        "            }\n",
        "rfr = RandomForestRegressor(n_jobs=-1,random_state=0)\n",
        "grid = perform_grid_search(rfr, params, custom_scorer, X_train, y_train)\n",
        "\n",
        "\n",
        "model = RandomForestRegressor(n_estimators = 500, \n",
        "                                criterion = \"squared_error\", # [\"squared_error\", \"absolute_error\", \"poisson\"],\n",
        "                                max_depth = 1,               # [None, 1, 2, 3],\n",
        "                                min_samples_split = 1.0,       # [1, 2, 3],\n",
        "                                max_features = \"sqrt\",      # [\"sqrt\", \"log2\", None]\n",
        "                                n_jobs=-1,\n",
        "                                random_state=0)\n",
        "model.fit(X_train, y_train)\n",
        "model.predict(X_test)\n",
        "\n",
        "# Évaluez le modèle en utilisant le scoreur personnalisé\n",
        "score = custom_scorer(model, X_test, y_test)\n",
        "print(\"\\nRMSE Score: %.2f$\" % score)\n",
        "print(\"Score: %.2f%%\" %model.score(X_test, y_test))\n",
        "\n",
        "# Évaluez le modèle en utilisant le cross_val\n",
        "rmse_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5,scoring=custom_scorer)\n",
        "cv_regressor_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5)\n",
        "print(\"\\nCV RMSE Score : %.2f$\" % np.mean(rmse_score))\n",
        "print(\"CV Score : %.2f%%\" % np.mean(cv_regressor_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2sDRKT_pUh8"
      },
      "source": [
        "### GridSearch on KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mF-uMVmpUh9",
        "outputId": "ac21962d-20f4-44b9-af7c-5923bcb06898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grid.best_score_:  -64204311.66584289\n",
            "grid.best_params_:  {'n_neighbors': 19, 'weights': 'uniform'}\n",
            "RMSE Score: -60336544.93$\n",
            "Score: 0.31%\n",
            "\n",
            "CV RMSE Score : -63255867.89$\n",
            "CV Score : 0.25%\n"
          ]
        }
      ],
      "source": [
        "## GridSearch\n",
        "# Définir les valeurs à tester pour les hyperparamètres du modèle\n",
        "params =    {'n_neighbors' : range(1, 50), \n",
        "            'weights' : ['uniform', 'distance']}\n",
        "knn = KNeighborsRegressor(n_jobs=-1)\n",
        "grid = perform_grid_search(knn, params, custom_scorer, X_train, y_train)\n",
        "\n",
        "# CHECK LES RESULTATS\n",
        "model = KNeighborsRegressor(n_neighbors= 43, weights= 'uniform', n_jobs=-1)\n",
        "model.fit(X_train, y_train)\n",
        "model.predict(X_test)\n",
        "\n",
        "# Évaluez le modèle en utilisant le scoreur personnalisé\n",
        "score = custom_scorer(model, X_test, y_test)\n",
        "print(\"RMSE Score: %.2f$\" % score)\n",
        "print(\"Score: %.2f%%\" %model.score(X_test, y_test))\n",
        "\n",
        "# Évaluez le modèle en utilisant le cross_val\n",
        "rmse_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5,scoring=custom_scorer)\n",
        "knn_regressor_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5)\n",
        "print(\"\\nCV RMSE Score : %.2f$\" % np.mean(rmse_score))\n",
        "print(\"CV Score : %.2f%%\" % np.mean(knn_regressor_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmi0SQeRpUh9"
      },
      "source": [
        "### Grid on MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Ccg4W3C8pUh9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "6acd48e7-5dc8-4899-8033-64f05fd715a0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-9ffeab2da704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_scorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-5ae0f79e4bd1>\u001b[0m in \u001b[0;36mperform_grid_search\u001b[0;34m(model_, params, scoring, X_train, y_train)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grid.best_score_: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grid.best_params_: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    565\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "params =    {'hidden_layer_sizes' : [(50,50,50), (100,5), (100,)],\n",
        "            'activation' : ['relu'],\n",
        "            'solver': ['adam'],\n",
        "            'alpha' : np.linspace(0.0001, 0.001, 10, endpoint = True),\n",
        "            'learning_rate_init': [0.001, 0.01, 0.1],\n",
        "            'max_iter': [500]}\n",
        "model = MLPRegressor(random_state=0)\n",
        "\n",
        "grid = perform_grid_search(model, params, custom_scorer, X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1roSalHrpUh-"
      },
      "source": [
        "### Training on MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIJIfZPmpUh-"
      },
      "outputs": [],
      "source": [
        "model = MLPRegressor(activation = 'relu', \n",
        "                    alpha = 0.0007, \n",
        "                    hidden_layer_sizes = (50, 50, 50), \n",
        "                    learning_rate_init = 0.1, \n",
        "                    max_iter = 500, \n",
        "                    solver = 'adam',\n",
        "                    random_state=0)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "model.predict(X_test)\n",
        "\n",
        "# Évaluez le modèle en utilisant le scoreur personnalisé\n",
        "score = custom_scorer(model, X_test, y_test)\n",
        "print(\"RMSE Score: %.2f$\" % score)\n",
        "print(\"Score: %.2f%%\" % model.score(X_test, y_test))\n",
        "\n",
        "# Évaluez le modèle en utilisant le cross_val\n",
        "rmse_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5,scoring=custom_scorer)\n",
        "mlp_regressor_score = cross_val_score(model, preprocessed_data, Y1.values.ravel(), cv=5)\n",
        "print(\"\\nCV RMSE Score : %.2f$\" % np.mean(rmse_score))\n",
        "print(\"CV Score : %.2f%%\" % np.mean(mlp_regressor_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2eeB4utpUh-"
      },
      "outputs": [],
      "source": [
        "# Score for each training subset\n",
        "N, train_score, val_score = learning_curve(model, X_train, y_train, cv=5)\n",
        "\n",
        "plt.plot(N, train_score.mean(axis=1), label='train')\n",
        "plt.plot(N, val_score.mean(axis=1), label='validation')\n",
        "plt.xlabel(\"train size\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08a_S2chpUh_"
      },
      "outputs": [],
      "source": [
        "# Evaluation metrics from the gridsearch\n",
        "res = pd.DataFrame(grid.cv_results_.values(), ).transpose()\n",
        "res.columns=grid.cv_results_.keys()\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network creation"
      ],
      "metadata": {
        "id": "mZc6SCiUpkuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = create_preprocessed(localsearch.best_solution.params)"
      ],
      "metadata": {
        "id": "4GF0I48Upj81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the number of inputs and outputs\n",
        "input_size = len(df.columns)\n",
        "output_size = 1\n",
        "\n",
        "# Define the number of hidden layers and the size of each layer\n",
        "num_hidden_layers = 2\n",
        "hidden_layer_size = 100\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the MLP model\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(input_size, hidden_layer_size),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(hidden_layer_size, hidden_layer_size),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(hidden_layer_size, hidden_layer_size),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(hidden_layer_size, output_size)\n",
        ").to(device)\n",
        "\n",
        "model = model.double()\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "lr = 1e-3\n",
        "betas = (0.9, 0.999)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Get the inputs and outputs as numpy arrays\n",
        "inputs = df.to_numpy()\n",
        "targets = Y1.to_numpy()\n",
        "\n",
        "# Convert the inputs and targets to PyTorch tensors\n",
        "inputs = torch.from_numpy(inputs).to(device).double()\n",
        "targets = torch.from_numpy(targets).to(device).double()\n",
        "\n",
        "\n",
        "# Train the model on a dataset of input-output pairs\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = []\n",
        "\n",
        "    # Split the dataset into batches of the specified size\n",
        "    for i in range(0, len(inputs)):\n",
        "        # Get the inputs and targets for the current batch\n",
        "        batch_inputs = inputs[i]\n",
        "        batch_targets = targets[i]\n",
        "\n",
        "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "        outputs = model.forward(batch_inputs)\n",
        "\n",
        "        # Compute and print the loss\n",
        "        loss = loss_fn(outputs, batch_targets)\n",
        "\n",
        "        # Add the loss to the list of train losses\n",
        "        train_loss.append(loss)\n",
        "\n",
        "        # Zero the gradients before running the backward pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass: compute gradient of the loss with respect to the model parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Call the optimizer to update the model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the average train loss for the current epoch\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {torch.sqrt(sum(train_loss)/len(train_loss))}\")\n"
      ],
      "metadata": {
        "id": "dt8bVrVRpajK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "9abed8abd2ce500fe15b078df5e406127fe6c9e241ddd0cff9b21749bf3b9f14"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}