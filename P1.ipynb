{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to loadinto a DataFrame\n",
    "# Y1.csv doesn’t have a header so\n",
    "# add one when loading the file\n",
    "X1 = pd.read_csv(\"data/X1.csv\")\n",
    "Y1 = pd.read_csv(\"data/Y1.csv\", header=None, names=['revenue'])\n",
    "\n",
    "\n",
    "# ENLEVER colonne \"Unnamed\" du dataset : utilité ? -> Pas listé dans les features du pdf\n",
    "X = X1.drop(['Unnamed: 0', 'img_url', 'description'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.describe())\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "corr = X.corr()\n",
    "heatmap = sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt='.1g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating empty DataFrame to start\n",
    "\"\"\"\n",
    "n_samples = X.shape[0]\n",
    "data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keeping the directly usable features\n",
    "\"\"\"\n",
    "def get_directrly_usable_features(df):\n",
    "    directly_usable_features = [\"ratings\", \"n_votes\", \"is_adult\"]\n",
    "    for feature in directly_usable_features:\n",
    "        df[feature] = X[feature]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"production_year\" feature\n",
    "\"\"\"\n",
    "\n",
    "def get_prod_year_feature(df, params):\n",
    "    \n",
    "    style = params[\"production_year_style\"] # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
    "    if style != \"no_period\" :\n",
    "        n_year_period = params[\"n_year_period\"]\n",
    "\n",
    "    # Removing previously computed categorie(s) for the \"production_year\" initial feature\n",
    "\n",
    "    for feature in df.columns:\n",
    "        if len(feature) >= 8 and (feature[:6] == \"period\" or feature == \"production_year\"):\n",
    "            df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "    # Creating new categorie(s) for the \"production_year\" initial feature\n",
    "\n",
    "    prod_year = X[\"production_year\"].copy()\n",
    "    if style == \"per_quantile\" or style == \"per_period_length\":\n",
    "        categories = np.ones((n_year_period, n_samples))\n",
    "\n",
    "        if style == \"per_quantile\":\n",
    "            thresholds = prod_year.quantile(np.arange(1, n_year_period) / n_year_period)\n",
    "        else :\n",
    "            thresholds = np.min(prod_year) + (np.max(prod_year) - np.min(prod_year))*np.arange(1, n_year_period)/n_year_period\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            categories[i+1] = (prod_year >= threshold).astype(int)\n",
    "            categories[i] -= categories[i+1]\n",
    "        for period in range(n_year_period):\n",
    "            df[\"period {}\".format(period)] = categories[period]\n",
    "    elif style == \"no_period\":\n",
    "        df[\"production_year\"] = prod_year\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"runtime\" feature\n",
    "\n",
    "The problem is here that we have some missing values, we have to deal with it.\n",
    "\"\"\"\n",
    "\n",
    "def get_runtime_feature(df, params):\n",
    "\n",
    "    # Add other smarter ways ?\n",
    "    \n",
    "    replace_type = params[\"runtime_replace_type\"] # \"zero\" / \"mean\"\n",
    "    \n",
    "    runtime = X[\"runtime\"].copy()\n",
    "    if replace_type == \"zero\":\n",
    "        runtime[runtime == \"\\\\N\"] = 0\n",
    "    if replace_type == \"mean\":\n",
    "        mean = np.mean(runtime[runtime != \"\\\\N\"].astype(float))\n",
    "        runtime[runtime == \"\\\\N\"] = mean\n",
    "    df[\"runtime\"] = runtime.astype(float)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"studio\" feature\n",
    "\n",
    "\n",
    "Juste rajouter toute les features une par une me semblait un peu lourd (yen a 509), ducoup j'effectue PCA dessus.\n",
    "Jsp si ça se fait ? (on peut changer l'algo de dimensionality reduction aussi si on veut)\n",
    "\"\"\"\n",
    "\n",
    "def get_studio_feature(df, params):\n",
    "    \n",
    "    \n",
    "    use_PCA = params[\"studio_use_PCA\"]\n",
    "    if use_PCA :\n",
    "        dim = params[\"studio_PCA_dim\"]\n",
    "\n",
    "\n",
    "    # Removing previously computed categorie(s) for the \"studio\" initial feature\n",
    "    for feature in df.columns:\n",
    "        if len(feature) >= 10 and feature[:10] == \"studio_PC_\":\n",
    "            df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "    # Creating new categorie(s) for the \"studio\" initial feature\n",
    "    studio = X[\"studio\"].copy()\n",
    "    studio_labels = np.unique(studio)\n",
    "    studio_features = np.zeros((len(studio_labels), n_samples))\n",
    "    for i, label in enumerate(studio_labels) :\n",
    "        studio_features[i] = (studio == label).astype(int)\n",
    "\n",
    "    # Applying pca or not\n",
    "    if use_PCA :\n",
    "        pca = PCA(n_components=dim)\n",
    "        out = pca.fit_transform(studio_features.T)\n",
    "    else :\n",
    "        out = studio_features.T\n",
    "\n",
    "    df[[\"studio_PC_{}\".format(i) for i in range(dim)]] = out\n",
    "    return df\n",
    "\n",
    "# Ya plein de warnings quand dim trop grand ou pas de PCA /: \n",
    "# jsp comment regler ça... en utilisant pd.concat ça tourne vraiment extrêmement lentement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"genres\" feature\n",
    "\n",
    "Je rajoute juste une feature par genre, j'espère ça suffit ? -> pt-être dimension reduction sur ça aussi ?\n",
    "Certains films ont pas de genre attitré (genre = \"\\\\N\"),\n",
    "ce que j'ai fait ici considère ça comme un genre à part entière, ptet on peut deal le truc autrement ?\n",
    "\"\"\"\n",
    "\n",
    "def get_genre_feature(df):\n",
    "\n",
    "    X.loc[X[\"genres\"] == \"\\\\N\", \"genres\"] = \"Others\"\n",
    "    all_genres = X[\"genres\"].copy()\n",
    "    diff_genres = []\n",
    "\n",
    "    for genres in np.unique(all_genres):\n",
    "        for genre in genres.split(\",\") :\n",
    "            if not genre in diff_genres :\n",
    "                diff_genres.append(genre)\n",
    "\n",
    "    for genre in diff_genres:\n",
    "        df[genre] = [1 if genre in genres.split(\",\") else 0 for genres in all_genres]  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"text_embeddings\" feature\n",
    "\n",
    "\n",
    "Dimension of embedding space is too high -> dimensionnality reduction\n",
    "J'utilise que PCA pr l'instant ici aussi\n",
    "\"\"\"\n",
    "def get_text_embedding_feature(df, params):\n",
    "    \n",
    "    output_dim = params[\"text_embedding_PCA_dim\"] # output dimension of PCA\n",
    "\n",
    "    # Removing previously computed categorie(s) for the \"text_embedding\" initial feature\n",
    "    for feature in df.columns:\n",
    "        if len(feature) >= 18 and feature[:18] == \"text_embedding_PC_\":\n",
    "            df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "    # Creating new categorie(s) for the \"text_embedding\" initial feature\n",
    "    text_embeddings = X[\"text_embeddings\"]\n",
    "    input_dim = 768\n",
    "    embeddings = np.zeros((n_samples, input_dim))\n",
    "    for i, text_embedding in enumerate(text_embeddings):\n",
    "        embeddings[i] = list(map(float,text_embedding[1:-1].split(\",\")))\n",
    "\n",
    "    # applying PCA\n",
    "    pca = PCA(n_components=output_dim)\n",
    "    output = pca.fit_transform(embeddings)\n",
    "\n",
    "    df[[\"text_embedding_PC_{}\".format(i) for i in range(output_dim)]] = output\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dealing with the \"img_embeddings\" feature\n",
    "\n",
    "\n",
    "Dimension of embedding space is too high -> dimensionnality reduction\n",
    "J'utilise que PCA pr l'instant ici aussi\n",
    "\"\"\"\n",
    "def get_img_embedding_feature(df, params):\n",
    "    output_dim = params[\"img_embedding_PCA_dim\"] # output dimension of PCA\n",
    "\n",
    "    # Removing previously computed categorie(s) for the \"img_embedding\" initial feature\n",
    "    for feature in df.columns:\n",
    "        if len(feature) >= 17 and feature[:17] == \"img_embedding_PC_\":\n",
    "            df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "    # Creating new categorie(s) for the \"img_embedding\" initial feature\n",
    "    img_embeddings = X[\"img_embeddings\"]\n",
    "    input_dim = 2048\n",
    "    embeddings = np.zeros((n_samples, input_dim))\n",
    "    for i, img_embedding in enumerate(img_embeddings):\n",
    "        embeddings[i] = list(map(float,img_embedding[1:-1].split(\",\")))\n",
    "\n",
    "    # applying PCA\n",
    "    pca = PCA(n_components=output_dim)\n",
    "    output = pca.fit_transform(embeddings)\n",
    "\n",
    "    df[[\"img_embedding_PC_{}\".format(i) for i in range(output_dim)]] = output\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessed(params):\n",
    "    df = pd.DataFrame()\n",
    "    df = get_directrly_usable_features(df)\n",
    "    df = get_genre_feature(df)\n",
    "    df = get_img_embedding_feature(df, params)\n",
    "    df = get_prod_year_feature(df, params)\n",
    "    df = get_runtime_feature(df, params)\n",
    "    df = get_studio_feature(df, params)\n",
    "    df = get_text_embedding_feature(df, params)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of data preprocessing\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"production_year_style\" : \"per_quantile\", # \"per_quantile\" / \"per_period_length\" / \"no_period\"\n",
    "    \"n_year_period\" : 5,\n",
    "    \"runtime_replace_type\" : \"mean\", # \"mean\" / \"zero\"\n",
    "    \"studio_use_PCA\" : True,\n",
    "    \"studio_PCA_dim\" : 50,\n",
    "    \"text_embedding_PCA_dim\" : 50,\n",
    "    \"img_embedding_PCA_dim\" : 50\n",
    "}\n",
    "\n",
    "preprocessed_data = create_preprocessed(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "corr = preprocessed_data.corr()\n",
    "\n",
    "heatmap = sns.heatmap(corr, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creer un data training/validation splités a partir du X1 (on garde X2 pour les vrais tests) \n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, Y1, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score computation : Root Mean Square Error\n",
    "\n",
    "def compute_rmse(predict, target):\n",
    "    return mean_squared_error(predict, target, squared=False)\n",
    "\n",
    "def compute_rmse2(predict, target):\n",
    "    if len(target.shape) == 2:\n",
    "        target = target.squeeze()\n",
    "    if len(predict.shape) == 2:\n",
    "        predict = predict.squeeze()\n",
    "    diff = target - predict\n",
    "    if len(diff.shape) == 1:\n",
    "        diff = np.expand_dims(diff, axis=-1)\n",
    "    rmse = np.sqrt(diff.T@diff / diff.shape[0])\n",
    "    return float(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(model_, params, scoring,X_train, y_train):\n",
    "\n",
    "    clf = Rando(model_, params, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    grid_result = clf.fit(X_train,y_train)\n",
    "    print(\"clf.best_score_: \", clf.best_score_)\n",
    "    print(\"clf.best_params_: \",clf.best_params_)\n",
    "    return grid_result\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(model_, params, scoring,X_train, y_train):\n",
    "\n",
    "    clf = GridSearchCV(model_, params, cv=5, scoring=scoring, n_jobs=-1)\n",
    "    grid_result = clf.fit(X_train,y_train)\n",
    "    print(\"clf.best_score_: \", clf.best_score_)\n",
    "    print(\"clf.best_params_: \",clf.best_params_)\n",
    "    return grid_result\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "scores = mutual_info_regression(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_with_features(X_train,Y_train, X_test, Y_test, selected_features):\n",
    "    X_train_filtered = X_train[selected_features]\n",
    "    X_test_filtered = X_test[selected_features]\n",
    "    \n",
    "    clf = LinearRegression(n_jobs=-1)\n",
    "    clf.fit(X_train_filtered,Y_train)\n",
    "    y_pred = clf.predict(X_test_filtered)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('By selecting features')\n",
    "# Dans ce cas, on selectionne tout\n",
    "Y_pred = fit_predict_with_features(X_train,y_train, X_test, y_test, X_train.columns)\n",
    "\n",
    "linear_regressor_score = cross_val_score(LinearRegression(), preprocessed_data, Y1, cv=5)\n",
    "print(np.mean(linear_regressor_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search on Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les valeurs à tester pour les hyperparamètres du modèle\n",
    "params =    {\"n_estimators\": [500, 700, 1000], \n",
    "            \"criterion\": [\"squared_error\", \"absolute_error\", \"poisson\"],\n",
    "            \"max_depth\": [None, 1, 2, 3],\n",
    "            \"min_samples_split\": [1, 2, 3],\n",
    "            \"max_features\" : [\"sqrt\", \"log2\", None]}\n",
    "\n",
    "knn = RandomForestRegressor(n_jobs=-1,random_state=0)\n",
    "scoring = \"neg_mean_squared_error\"\n",
    "\n",
    "grid = perform_grid_search(knn, params, scoring, X_train, y_train)\n",
    "pred = grid.best_estimator_.predict(X_test)\n",
    "print(\"pred: \",pred.mean())\n",
    "\n",
    "print(\"Score\")\n",
    "model = grid.best_estimator_\n",
    "print(model.score(X_test,y_test))\n",
    "rmse = compute_rmse(pred, y_test)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch on KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les valeurs à tester pour les hyperparamètres du modèle\n",
    "params =    {'n_neighbors' : range(3, 40), \n",
    "            'weights' : ['uniform', 'distance']}\n",
    "knn = KNeighborsRegressor(n_jobs=-1)\n",
    "scoring = \"neg_mean_squared_error\"\n",
    "\n",
    "grid = perform_grid_search(knn, params, scoring, X_train, y_train)\n",
    "pred = grid.best_estimator_.predict(X_test)\n",
    "print(\"pred: \",pred.mean())\n",
    "\n",
    "print(\"Score\")\n",
    "model = grid.best_estimator_\n",
    "print(model.score(X_test,y_test))\n",
    "rmse = compute_rmse(pred, y_test)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for each training subset\n",
    "N, train_score, val_score = learning_curve(knn, X_train, y_train, cv=5)\n",
    "\n",
    "plt.plot(N, train_score.mean(axis=1), label='train')\n",
    "plt.plot(N, val_score.mean(axis=1), label='validation')\n",
    "plt.xlabel(\"train size\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics from the gridsearch\n",
    "res = pd.DataFrame(grid.cv_results_.values(), ).transpose()\n",
    "res.columns=grid.cv_results_.keys()\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9abed8abd2ce500fe15b078df5e406127fe6c9e241ddd0cff9b21749bf3b9f14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
